{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## AzureML Model Monitoring through Operationalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this sample notebook, you will observe the end-to-end lifecycle of the Machine Learning (ML) operationalization process. You will follow the following steps to train your ML model, deploy it to production, and monitor it to ensure its continuous performance:\n",
        "\n",
        "1) Setup environment \n",
        "2) Register data assets\n",
        "3) Train the model\n",
        "4) Deploy the model\n",
        "5) Simulate inference requests\n",
        "6) Monitor the model\n",
        "\n",
        "Let's begin. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup your environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To start, connect to your project workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756980823821
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# Connect to the project workspace\n",
        "ml_client = MLClient.from_config(credential=DefaultAzureCredential())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up a compute cluster to use to train your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756978817754
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "cluster_basic = AmlCompute(\n",
        "    name=\"cpu-cluster\",\n",
        "    type=\"amlcompute\",\n",
        "    size=\"STANDARD_F2S_V2\",  # you can replace it with other supported VM SKUs\n",
        "    location=ml_client.workspaces.get(ml_client.workspace_name).location,\n",
        "    min_instances=0,\n",
        "    max_instances=1,\n",
        "    idle_time_before_scale_down=360,\n",
        ")\n",
        "\n",
        "ml_client.begin_create_or_update(cluster_basic).result()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Register data assets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, let's use some sample data to train our model. We will randomly split the dataset into reference and production sets. We add a timestamp column to simulate \"production-like\" data, since production data typically comes with timestamps. The dataset we are using in this example notebook has several columns related to credit card borrowers and contains a column on whether or not they defaulted on their credit card debt. We will train a model to predict `DEFAULT_NEXT_MONTH`, which is whether or not a borrower will default on their debt next month."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756978819354
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "# Read the default_of_credit_card_clients dataset into a pandas data frame\n",
        "data_path = \"https://azuremlexamples.blob.core.windows.net/datasets/credit_card/default_of_credit_card_clients.csv\"\n",
        "df = pd.read_csv(data_path, header=1, index_col=0).rename(\n",
        "    columns={\"default payment next month\": \"DEFAULT_NEXT_MONTH\"}\n",
        ")\n",
        "\n",
        "# Split the data into production_data_df and reference_data_df\n",
        "# Use the iloc method to select the first 80% and the last 20% of the rows\n",
        "reference_data_df = df.iloc[: int(0.8 * len(df))].copy()\n",
        "production_data_df = df.iloc[int(0.8 * len(df)) :].copy()\n",
        "\n",
        "# Add a timestamp column in ISO8601 format\n",
        "timestamp = datetime.datetime.now() - datetime.timedelta(days=45)\n",
        "reference_data_df[\"TIMESTAMP\"] = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
        "production_data_df[\"TIMESTAMP\"] = [\n",
        "    timestamp + datetime.timedelta(minutes=i * 10)\n",
        "    for i in range(len(production_data_df))\n",
        "]\n",
        "production_data_df[\"TIMESTAMP\"] = production_data_df[\"TIMESTAMP\"].apply(\n",
        "    lambda x: x.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756978819880
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "def write_df(df, local_path, file_name):\n",
        "    # Create directory if it does not exist\n",
        "    os.makedirs(local_path, exist_ok=True)\n",
        "\n",
        "    # Write data\n",
        "    df.to_csv(f\"{local_path}/{file_name}\", index=False)\n",
        "\n",
        "\n",
        "# Write data to local directory\n",
        "reference_data_dir_local_path = \"../data/reference\"\n",
        "production_data_dir_local_path = \"../data/production\"\n",
        "\n",
        "write_df(reference_data_df, reference_data_dir_local_path, \"01.csv\"),\n",
        "write_df(production_data_df, production_data_dir_local_path, \"01.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756978832608
        }
      },
      "outputs": [],
      "source": [
        "import mltable\n",
        "from mltable import MLTableHeaders, MLTableFileEncoding\n",
        "\n",
        "from azureml.fsspec import AzureMachineLearningFileSystem\n",
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "\n",
        "def upload_data_and_create_data_asset(\n",
        "    local_path, remote_path, datastore_uri, data_name, data_version\n",
        "):\n",
        "    # Write MLTable file\n",
        "    tbl = mltable.from_delimited_files(\n",
        "        paths=[{\"pattern\": f\"{datastore_uri}{remote_path}*.csv\"}],\n",
        "        delimiter=\",\",\n",
        "        header=\"all_files_same_headers\",\n",
        "        infer_column_types=True,\n",
        "        include_path_column=False,\n",
        "        encoding=\"utf8\",\n",
        "    )\n",
        "\n",
        "    tbl.save(local_path)\n",
        "\n",
        "    # Instantiate file system\n",
        "    fs = AzureMachineLearningFileSystem(datastore_uri)\n",
        "\n",
        "    # Upload data\n",
        "    fs.upload(\n",
        "        lpath=local_path,\n",
        "        rpath=remote_path,\n",
        "        recursive=False,\n",
        "        **{\"overwrite\": \"MERGE_WITH_OVERWRITE\"},\n",
        "    )\n",
        "\n",
        "    # Define the Data asset object\n",
        "    data = Data(\n",
        "        path=f\"{datastore_uri}{remote_path}\",\n",
        "        type=AssetTypes.MLTABLE,\n",
        "        name=data_name,\n",
        "        version=data_version,\n",
        "    )\n",
        "\n",
        "    # Create the data asset in the workspace\n",
        "    ml_client.data.create_or_update(data)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# Datastore uri for data\n",
        "datastore_uri = \"azureml://subscriptions/{}/resourcegroups/{}/workspaces/{}/datastores/workspaceblobstore/paths/\".format(\n",
        "    ml_client.subscription_id, ml_client.resource_group_name, ml_client.workspace_name\n",
        ")\n",
        "\n",
        "# Define paths\n",
        "reference_data_dir_remote_path = \"data/credit-default/reference/\"\n",
        "production_data_dir_remote_path = \"data/credit-default/production/\"\n",
        "\n",
        "# Define data asset names\n",
        "reference_data_asset_name = \"credit-default-reference\"\n",
        "production_data_asset_name = \"credit-default-production\"\n",
        "\n",
        "# Write data to remote directory and create data asset\n",
        "reference_data = upload_data_and_create_data_asset(\n",
        "    reference_data_dir_local_path,\n",
        "    reference_data_dir_remote_path,\n",
        "    datastore_uri,\n",
        "    reference_data_asset_name,\n",
        "    \"1\",\n",
        ")\n",
        "production_data = upload_data_and_create_data_asset(\n",
        "    production_data_dir_local_path,\n",
        "    production_data_dir_remote_path,\n",
        "    datastore_uri,\n",
        "    production_data_asset_name,\n",
        "    \"1\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756978867728
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import load_job\n",
        "\n",
        "# Define training pipeline directory\n",
        "training_pipeline_path = \"../configurations/training_pipeline.yaml\"\n",
        "\n",
        "# Trigger training\n",
        "training_pipeline_definition = load_job(source=training_pipeline_path)\n",
        "training_pipeline_job = ml_client.jobs.create_or_update(training_pipeline_definition)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756979863791
        }
      },
      "outputs": [],
      "source": [
        "ml_client.jobs.stream(training_pipeline_job.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Deploy the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Deploy the model with AzureML managed online endpoints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create Batch Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756980833124
        }
      },
      "outputs": [],
      "source": [
        "from time import sleep\n",
        "from azure.ai.ml import MLClient, Input\n",
        "from azure.ai.ml.entities import (\n",
        "    BatchEndpoint,\n",
        "    ModelBatchDeployment,\n",
        "    ModelBatchDeploymentSettings,\n",
        "    Model,\n",
        "    AmlCompute,\n",
        "    BatchRetrySettings,\n",
        "    CodeConfiguration,\n",
        "    Environment,\n",
        ")\n",
        "from azure.ai.ml.constants import AssetTypes, BatchDeploymentOutputAction\n",
        "from azure.identity import DefaultAzureCredential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756979987356
        }
      },
      "outputs": [],
      "source": [
        "endpoint_name = \"BATCH_ENDPOINT_NAME\"\n",
        "endpoint = BatchEndpoint(\n",
        "    name=endpoint_name,\n",
        "    description=\"A custom batch endpoint for inference\",\n",
        ")\n",
        "ml_client.batch_endpoints.begin_create_or_update(endpoint).result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756979997328
        }
      },
      "outputs": [],
      "source": [
        "compute_name = \"batch-cluster\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756980030643
        }
      },
      "outputs": [],
      "source": [
        "compute_name = \"batch-cluster\"\n",
        "if not any(filter(lambda m: m.name == compute_name, ml_client.compute.list())):\n",
        "    compute_cluster = AmlCompute(\n",
        "        name=compute_name, description=\"amlcompute\", min_instances=0, max_instances=5\n",
        "    )\n",
        "    ml_client.begin_create_or_update(compute_cluster).result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756980160248
        }
      },
      "outputs": [],
      "source": [
        "environment = Environment(\n",
        "    name=\"batch-custom-env\",\n",
        "    conda_file=\"../environments/score.yaml\",\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756980162659
        }
      },
      "outputs": [],
      "source": [
        "model = ml_client.models.get(\n",
        "    name=\"credit-default-model\",\n",
        "    version=\"1\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756980164837
        }
      },
      "outputs": [],
      "source": [
        "deployment = ModelBatchDeployment(\n",
        "    name=\"batch_deployment_name\",\n",
        "    description=\"batch deployment\",\n",
        "    endpoint_name=endpoint.name,\n",
        "    model=model,\n",
        "    environment=environment,\n",
        "    code_configuration=CodeConfiguration(\n",
        "        code=\"../code\",\n",
        "        scoring_script=\"batch_driver.py\",\n",
        "    ),\n",
        "    compute=compute_name,\n",
        "    settings=ModelBatchDeploymentSettings(\n",
        "        mini_batch_size=1,\n",
        "        output_action=BatchDeploymentOutputAction.SUMMARY_ONLY,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756980237651
        }
      },
      "outputs": [],
      "source": [
        "ml_client.batch_deployments.begin_create_or_update(deployment).result()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Simulate production inference data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Generate Sample Data\n",
        "\n",
        "We generate sample inference data by taking the distribution for each input feature and adding a small amount of random noise. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756834184517
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "# Define numeric and categotical feature columns\n",
        "NUMERIC_FEATURES = [\n",
        "    \"LIMIT_BAL\",\n",
        "    \"AGE\",\n",
        "    \"BILL_AMT1\",\n",
        "    \"BILL_AMT2\",\n",
        "    \"BILL_AMT3\",\n",
        "    \"BILL_AMT4\",\n",
        "    \"BILL_AMT5\",\n",
        "    \"BILL_AMT6\",\n",
        "    \"PAY_AMT1\",\n",
        "    \"PAY_AMT2\",\n",
        "    \"PAY_AMT3\",\n",
        "    \"PAY_AMT4\",\n",
        "    \"PAY_AMT5\",\n",
        "    \"PAY_AMT6\",\n",
        "]\n",
        "CATEGORICAL_FEATURES = [\n",
        "    \"SEX\",\n",
        "    \"EDUCATION\",\n",
        "    \"MARRIAGE\",\n",
        "    \"PAY_0\",\n",
        "    \"PAY_2\",\n",
        "    \"PAY_3\",\n",
        "    \"PAY_4\",\n",
        "    \"PAY_5\",\n",
        "    \"PAY_6\",\n",
        "]\n",
        "\n",
        "\n",
        "def generate_sample_inference_data(df_production, number_of_records=20):\n",
        "    # Sample records\n",
        "    df_sample = df_production.sample(n=number_of_records, replace=True)\n",
        "\n",
        "    # Generate numeric features with random noise\n",
        "    df_numeric_generated = pd.DataFrame(\n",
        "        {\n",
        "            feature: np.random.normal(\n",
        "                0, df_production[feature].std(), number_of_records\n",
        "            ).astype(np.int64)\n",
        "            for feature in NUMERIC_FEATURES\n",
        "        }\n",
        "    ) + df_sample[NUMERIC_FEATURES].reset_index(drop=True)\n",
        "\n",
        "    # Take categorical columns\n",
        "    df_categorical = df_sample[CATEGORICAL_FEATURES].reset_index(drop=True)\n",
        "\n",
        "    # Combine numerical and categorical columns\n",
        "    df_combined = pd.concat([df_numeric_generated, df_categorical], axis=1)\n",
        "    # Add a timestamp column in ISO8601 format\n",
        "    timestamp = datetime.datetime.now() + datetime.timedelta(days=5)\n",
        "    df_combined[\"TIMESTAMP\"] = [\n",
        "        timestamp + datetime.timedelta(minutes=i * 10)\n",
        "        for i in range(len(df_combined))\n",
        "    ]\n",
        "    df_combined[\"TIMESTAMP\"] = df_combined[\"TIMESTAMP\"].apply(\n",
        "        lambda x: x.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
        "    )\n",
        "\n",
        "    return df_combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756834186698
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import mltable\n",
        "import pandas as pd\n",
        "from azure.ai.ml import MLClient\n",
        "import datetime\n",
        "\n",
        "# Load production / inference data\n",
        "data_asset = ml_client.data.get(\"credit-default-production\", version=\"1\")\n",
        "tbl = mltable.load(data_asset.path)\n",
        "df_production = tbl.to_pandas_dataframe()\n",
        "\n",
        "# Generate sample data for inference\n",
        "number_of_records = 40\n",
        "os.makedirs(\"../batch_data\", exist_ok=True)\n",
        "for i in range(5):\n",
        "    df_generated = generate_sample_inference_data(df_production, number_of_records)\n",
        "    df_generated.to_csv(f\"../batch_data/batch_{i}.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Call batch endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756980838426
        }
      },
      "outputs": [],
      "source": [
        "endpoint_name = \"BATCH_ENDPOINT_NAME\"\n",
        "endpoint = ml_client.batch_endpoints.get(endpoint_name)\n",
        "deployment_name=\"BATCH_DEPLOYMENT_NAME\"\n",
        "deployment = ml_client.batch_deployments.get(deployment_name, endpoint_name)\n",
        "\n",
        "print(endpoint.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756980839423
        }
      },
      "outputs": [],
      "source": [
        "input = Input(\n",
        "    type=AssetTypes.URI_FOLDER,\n",
        "    path=\"../batch_data/\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756980851957
        }
      },
      "outputs": [],
      "source": [
        "job = ml_client.batch_endpoints.invoke(\n",
        "    endpoint_name=endpoint.name, deployment_name=deployment.name, input=input\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756980852194
        }
      },
      "outputs": [],
      "source": [
        "job.name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756981868200
        }
      },
      "outputs": [],
      "source": [
        "scoring_job = list(ml_client.jobs.list(parent_job_name=job.name))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756981870407
        }
      },
      "outputs": [],
      "source": [
        "scoring_job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756981877015
        }
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"../scoring_output\", exist_ok=True)\n",
        "ml_client.jobs.download(name=scoring_job.name, download_path=\"../scoring_output/\", output_name=\"score\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1756981880779
        }
      },
      "outputs": [],
      "source": [
        "from azureml.fsspec import AzureMachineLearningFileSystem\n",
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "\n",
        "# Datastore uri for data\n",
        "datastore_uri = \"azureml://subscriptions/{}/resourcegroups/{}/workspaces/{}/datastores/workspaceblobstore/paths/\".format(\n",
        "    ml_client.subscription_id, ml_client.resource_group_name, ml_client.workspace_name\n",
        ")\n",
        "\n",
        "# Define paths\n",
        "output_data_dir_remote_path = \"data/credit-default/output/\"\n",
        "\n",
        "# Instantiate file system\n",
        "fs = AzureMachineLearningFileSystem(datastore_uri)\n",
        "\n",
        "# Upload data\n",
        "fs.upload(\n",
        "    lpath=\"../scoring_output/named-outputs/score/\",\n",
        "    rpath=output_data_dir_remote_path,\n",
        "    recursive=False,\n",
        "    **{\"overwrite\": \"MERGE_WITH_OVERWRITE\"},\n",
        ")\n",
        "output_data_asset_name=\"credit-default-output-folder\"\n",
        "\n",
        "# Define the Data asset object\n",
        "data = Data(\n",
        "    path=f\"{datastore_uri}{output_data_dir_remote_path}\",\n",
        "    type=AssetTypes.URI_FOLDER,\n",
        "    name=output_data_asset_name,\n",
        "    version=\"2\",\n",
        ")\n",
        "\n",
        "# Create the data asset in the workspace\n",
        "ml_client.data.create_or_update(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create preprocessing component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml import load_component\n",
        "parent_dir = \"..\"\n",
        "spec_file = load_component(source=parent_dir + \"components/custom_preprocessing/spec.yaml\")\n",
        "print(spec_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    # try get back the defined component\n",
        "    spec_file = ml_client.components.get(name=spec_file.name, version=spec_file.version)\n",
        "except:\n",
        "    # create if not exists\n",
        "    spec_file = ml_client.components.create_or_update(spec_file)\n",
        "\n",
        "print(spec_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create the Schedule pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This can be done via running the file batch_endpoint_monitoring.yaml via a command `az ml schedule create -f batch_endpoint_monitoring.yaml --subscription <subscription_id> --workspace <workspace> --resource-group <resource_group>`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
